{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/fbcotter/pytorch_wavelets PyWavelets open_clip_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import open_clip\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "from arroz import Diffuzz, VQModel, DiffusionModel, PriorModel, to_latent, from_latent\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_checkpoint_path = \"checkpoints/clip2img_v1_288k_ema.pt\"\n",
    "prior_checkpoint_path = \"checkpoints/prior_v1_352k_ema.pt\"\n",
    "vqgan_checkpoint_path = \"checkpoints/vqwatercolor_v1.pt\"\n",
    "clip_model_name = ('ViT-H-14', 'laion2b_s32b_b79k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Models & Tools \n",
    "Running this cell might take a while as every model is loaded into memory.\n",
    "We need:\n",
    "- **diffuzz**: To handle the sampling process\n",
    "- **vqmodel**: To decode the sampled image latents\n",
    "- **clip_model**: To generate the CLIP text embeddings from our prompt\n",
    "- **generator**: To convert our sampled CLIP image embeddings into image latents\n",
    "- **prior**: To sample CLIP image embeddings from CLIP text embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - diffusion tool -\n",
    "diffuzz = Diffuzz(device=device)\n",
    "\n",
    "# - vqgan -\n",
    "vqmodel = VQModel().to(device)\n",
    "vqmodel.load_state_dict(torch.load(vqgan_checkpoint_path, map_location=device))\n",
    "vqmodel.eval().requires_grad_(False)\n",
    "\n",
    "# - openclip -\n",
    "clip_model, _, _ = open_clip.create_model_and_transforms(clip_model_name[0], pretrained=clip_model_name[1],  device=device)\n",
    "clip_model.eval().requires_grad_(False)\n",
    "clip_tokenizer = open_clip.get_tokenizer(clip_model_name[0])\n",
    "\n",
    "# - diffusion models - \n",
    "generator = DiffusionModel().to(device)\n",
    "generator.load_state_dict(torch.load(generator_checkpoint_path, map_location=device))\n",
    "generator.eval().requires_grad_(False)\n",
    "\n",
    "prior = PriorModel().to(device)\n",
    "prior.load_state_dict(torch.load(prior_checkpoint_path, map_location=device))\n",
    "prior.eval().requires_grad_(False)\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text2Image Sampling\n",
    "This requires 2 sampling stages:\n",
    "1. Sampling the CLIP image embedding from the CLIP text embedding using the prior\n",
    "2. Sampling image latents embedding from the CLIP image embedding using the generator\n",
    "\n",
    "Then the latents are decoded into an image using the vqGAN decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Closeup studio photography of an old vietnamese woman'\n",
    "batch_size = 4\n",
    "\n",
    "prior_timesteps = 60\n",
    "prior_cfg = 3.0\n",
    "prior_sampler = 'ddpm'\n",
    "\n",
    "clip2img_timesteps = 20\n",
    "clip2img_cfg = 7.0\n",
    "clip2img_sampler = 'ddim'\n",
    "\n",
    "clip_embedding_shape = (batch_size, 1024)\n",
    "image_latent_shape = (batch_size, 4, 64, 64)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    with torch.autocast(device_type=\"cuda\"):\n",
    "        with torch.random.fork_rng():\n",
    "            torch.manual_seed(42) # For reproducibility\n",
    "            \n",
    "            # prompt to CLIP embeddings\n",
    "            captions = [prompt] * batch_size\n",
    "            captions = clip_tokenizer(captions).to(device)\n",
    "            text_embeddings = clip_model.encode_text(captions).float()\n",
    "            \n",
    "            # sample image embedding with prior\n",
    "            sampled_image_embeddings = diffuzz.sample(\n",
    "                prior, {'c': text_embeddings}, clip_embedding_shape,\n",
    "                timesteps=prior_timesteps, cfg=prior_cfg, sampler=prior_sampler\n",
    "            )[-1]\n",
    "            \n",
    "            # sample image latents\n",
    "            sampled = diffuzz.sample(\n",
    "                generator, {'c': sampled_image_embeddings}, image_latent_shape,\n",
    "                timesteps=clip2img_timesteps, cfg=clip2img_cfg, sampler=clip2img_sampler\n",
    "            )[-1]\n",
    "            \n",
    "            # decode sampled latents\n",
    "            sampled_images = from_latent(sampled, vqmodel).clamp(0, 1)\n",
    "            \n",
    "plt.figure(figsize=(32, 32))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(torch.cat([\n",
    "    torch.cat([i for i in sampled_images.cpu()], dim=-1)\n",
    "], dim=-2).permute(1, 2, 0))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
